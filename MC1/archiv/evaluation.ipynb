{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bcd40bd",
   "metadata": {},
   "source": [
    "Textcleaner: das mittlere nehmen\n",
    "nur 1000-2000 Zeilen\n",
    "2 Splitter, 2 Similarity\n",
    "anderes Modell nehmen zum evaluieren (1 Variante) -> 4 müssen noch rausgenommen werden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5826ce8d",
   "metadata": {},
   "source": [
    "1. Question\n",
    "2. Answer to Evaluate\n",
    "3. Context to Evaluate\n",
    "4. Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44895a6",
   "metadata": {},
   "source": [
    "## Evaluierung Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cbf879",
   "metadata": {},
   "source": [
    "- mit samples arbeiten von question, answer, contexts, ground truth\n",
    "- score berechnen (Faithfulness ist eine Formel -> hoch, dann ist der Kontext zum Groundtruth hoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a5d39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/jk3grg717z34yf9xh85kj0bh0000gn/T/ipykernel_33294/3406113087.py:8: LangChainDeprecationWarning: The class `AzureOpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import AzureOpenAIEmbeddings``.\n",
      "  embedding_model = AzureOpenAIEmbeddings(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'chunk_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Embedding-Funktion (muss identisch sein wie beim Erstellen der DB!)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m embedding_model = \u001b[43mAzureOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mazure_endpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mAZURE_OPENAI_ENDPOINT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mAZURE_OPENAI_API_KEY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2023-05-15\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdeployment\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-embedding-3-large\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Muss mit dem Deployment-Namen übereinstimmen\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Pfad zur zu ladenden DB\u001b[39;00m\n\u001b[32m     16\u001b[39m persist_directory = Path.cwd().parent / \u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mchroma_recchar_800_cosine\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/gitlab.fhnw.ch/gruppen_arbeit/npr_fs25/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:221\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    219\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    220\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/gitlab.fhnw.ch/gruppen_arbeit/npr_fs25/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:221\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    219\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    220\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/gitlab.fhnw.ch/gruppen_arbeit/npr_fs25/.venv/lib/python3.12/site-packages/langchain_community/embeddings/azure_openai.py:105\u001b[39m, in \u001b[36mAzureOpenAIEmbeddings.validate_environment\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m     99\u001b[39m values[\u001b[33m\"\u001b[39m\u001b[33mazure_ad_token\u001b[39m\u001b[33m\"\u001b[39m] = values.get(\u001b[33m\"\u001b[39m\u001b[33mazure_ad_token\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m os.getenv(\n\u001b[32m    100\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAZURE_OPENAI_AD_TOKEN\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m )\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Azure OpenAI embedding models allow a maximum of 2048 texts\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# at a time in each batch\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# See: https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#embeddings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m values[\u001b[33m\"\u001b[39m\u001b[33mchunk_size\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mmin\u001b[39m(\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchunk_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[32m2048\u001b[39m)\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'chunk_size'"
     ]
    }
   ],
   "source": [
    "from credentials import AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "import chromadb\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from pathlib import Path\n",
    "\n",
    "# Embedding-Funktion (muss identisch sein wie beim Erstellen der DB!)\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=\"2023-05-15\",\n",
    "    deployment=\"text-embedding-3-large\"  # Muss mit dem Deployment-Namen übereinstimmen\n",
    ")\n",
    "\n",
    "# Pfad zur zu ladenden DB\n",
    "persist_directory = Path.cwd().parent / \"src\" / \"chroma_recchar_800_cosine\"\n",
    "\n",
    "collection_name = \"recchar_800_cosine\"\n",
    "\n",
    "# Client und Collection prüfen\n",
    "client = chromadb.PersistentClient(path=str(persist_directory))\n",
    "collection_names = client.list_collections()\n",
    "\n",
    "if collection_name not in collection_names:\n",
    "    raise ValueError(f\"Collection '{collection_name}' nicht gefunden in {persist_directory}.\")\n",
    "\n",
    "print(f\"Collection '{collection_name}' gefunden. Laden...\")\n",
    "\n",
    "# Vector Store laden\n",
    "vector_store = Chroma(\n",
    "    embedding_function=embedding_model,\n",
    "    collection_name=collection_name,\n",
    "    persist_directory=str(persist_directory)\n",
    ")\n",
    "\n",
    "# Dokument-Anzahl prüfen\n",
    "doc_count = vector_store._collection.count()\n",
    "print(f\"Datenbank geladen: {doc_count} Dokumente gefunden.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b091caea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/husazwerg/code/gitlab.fhnw.ch/gruppen_arbeit/npr_fs25/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-30 10:17:54,503 - INFO - Lade Testdaten aus Parquet-Datei ...\n",
      "2025-04-30 10:17:54,760 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Collection 'chroma_recchar_800_cosine' nicht gefunden.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RAGAS_Evaluator\n\u001b[32m      3\u001b[39m evaluator = RAGAS_Evaluator()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m results, ragas_metrics, ir_metrics = \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchroma_recchar_800_cosine\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretriever_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msimilarity_k5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msimple\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_output.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRAGAS:\u001b[39m\u001b[33m\"\u001b[39m, ragas_metrics)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIR:\u001b[39m\u001b[33m\"\u001b[39m, ir_metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/gitlab.fhnw.ch/gruppen_arbeit/npr_fs25/MC1/src/evaluation.py:185\u001b[39m, in \u001b[36mRAGAS_Evaluator.run\u001b[39m\u001b[34m(self, collection_name, retriever_name, prompt_name, temperature, save_path)\u001b[39m\n\u001b[32m    182\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[33mLade Testdaten aus Parquet-Datei ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    183\u001b[39m df = pd.read_parquet(\u001b[38;5;28mself\u001b[39m.data_path)\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m vector_store = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_vectorstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m retrievers = \u001b[38;5;28mself\u001b[39m.get_retrievers(vector_store)\n\u001b[32m    187\u001b[39m prompts = \u001b[38;5;28mself\u001b[39m.get_prompts()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/gitlab.fhnw.ch/gruppen_arbeit/npr_fs25/MC1/src/evaluation.py:46\u001b[39m, in \u001b[36mRAGAS_Evaluator.load_vectorstore\u001b[39m\u001b[34m(self, collection_name)\u001b[39m\n\u001b[32m     43\u001b[39m client = chromadb.PersistentClient(path=\u001b[38;5;28mstr\u001b[39m(persist_directory))\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collection_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m client.list_collections():\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCollection \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m nicht gefunden.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVektorstore \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m geladen.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Chroma(embedding_function=embedding_model, collection_name=collection_name, persist_directory=\u001b[38;5;28mstr\u001b[39m(persist_directory))\n",
      "\u001b[31mValueError\u001b[39m: Collection 'chroma_recchar_800_cosine' nicht gefunden."
     ]
    }
   ],
   "source": [
    "from evaluation import RAGAS_Evaluator\n",
    "\n",
    "evaluator = RAGAS_Evaluator()\n",
    "\n",
    "results, ragas_metrics, ir_metrics = evaluator.run(\n",
    "    collection_name=\"chroma_recchar_800_cosine\",\n",
    "    retriever_name=\"similarity_k5\",\n",
    "    prompt_name=\"simple\",\n",
    "    temperature=0.1,\n",
    "    save_path=\"eval_output.csv\"\n",
    ")\n",
    "\n",
    "print(\"RAGAS:\", ragas_metrics)\n",
    "print(\"IR:\", ir_metrics)\n",
    "print(\"Durchschnittliche Antwortähnlichkeit:\", results[\"similarity_to_gold\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410052f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_eval_result(self, df: pd.DataFrame):\n",
    "\n",
    "    sns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "\n",
    "    columns = [\n",
    "        \"faithfulness\",\n",
    "        \"answer_relevancy\",\n",
    "        \"context_precision\",\n",
    "        \"context_entity_recall\",\n",
    "        \"answer_similarity\",\n",
    "        \"answer_correctness\",\n",
    "    ]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=df[columns], palette=\"Set1\", width=0.6, linewidth=1.5)\n",
    "    plt.title(f\"{self.name}: Ragas Metrics boxplot\", fontsize=16)\n",
    "    plt.ylabel(\"Score\", fontsize=14)\n",
    "    plt.xticks(fontsize=12, rotation=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_eval_result_bar(self, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Plot a barplot of evaluation scores for RAGAS metrics, MRR, precision@2, and recall@2.\n",
    "    Args:\n",
    "        results (pd.DataFrame): The results DataFrame from the evaluation.\n",
    "    \"\"\"\n",
    "    sns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "\n",
    "    ragas_metrics = [\n",
    "        \"faithfulness\",\n",
    "        \"answer_relevancy\",\n",
    "        \"context_precision\",\n",
    "        \"context_entity_recall\",\n",
    "        \"semantic_similarity\",\n",
    "        \"answer_correctness\",\n",
    "    ]\n",
    "    additional_metrics = [\"MRR\", \"precision@2\", \"recall@2\"]\n",
    "    all_metrics = ragas_metrics + additional_metrics\n",
    "\n",
    "    metric_means = df[all_metrics].mean()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(\n",
    "        x=metric_means.index,\n",
    "        y=metric_means.values,\n",
    "        palette=\"Set1\",\n",
    "        hue=metric_means.index,\n",
    "        legend=False,\n",
    "    )\n",
    "    plt.title(f\"{self.name}: Ragas + Non-LLM Metrics (Mean)\", fontsize=16)\n",
    "    plt.ylabel(\"Mean Score\", fontsize=14)\n",
    "    plt.xlabel(\"Metrics\", fontsize=14)\n",
    "    plt.xticks(fontsize=12, rotation=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_results_all(self, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Display both the boxplot for RAGAS metrics and the barplot for RAGAS + non-LLM metrics.\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing raw RAGAS metric values for the boxplot.\n",
    "        results (pd.DataFrame): DataFrame containing RAGAS and non-LLM metric scores for the barplot.\n",
    "    \"\"\"\n",
    "    sns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "\n",
    "    # Boxplot: RAGAS Metrics\n",
    "    boxplot_columns = [\n",
    "        \"faithfulness\",\n",
    "        \"answer_relevancy\",\n",
    "        \"context_precision\",\n",
    "        \"context_entity_recall\",\n",
    "        \"semantic_similarity\",\n",
    "        \"answer_correctness\",\n",
    "    ]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=df[boxplot_columns], palette=\"Set1\", width=0.6, linewidth=1.5)\n",
    "    plt.title(f\"{self.name}: RAGAS Metrics Boxplot\", fontsize=16)\n",
    "    plt.ylabel(\"Score\", fontsize=14)\n",
    "    plt.xticks(fontsize=12, rotation=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Barplot: RAGAS + Non-LLM Metrics\n",
    "    ragas_metrics = [\n",
    "        \"faithfulness\",\n",
    "        \"answer_relevancy\",\n",
    "        \"context_precision\",\n",
    "        \"context_entity_recall\",\n",
    "        \"semantic_similarity\",\n",
    "        \"answer_correctness\",\n",
    "    ]\n",
    "    additional_metrics = [\"MRR\", \"precision@2\", \"recall@2\"]\n",
    "    all_metrics = ragas_metrics + additional_metrics\n",
    "\n",
    "    metric_means = df[all_metrics].mean()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(\n",
    "        x=metric_means.index,\n",
    "        y=metric_means.values,\n",
    "        palette=\"Set1\",\n",
    "        hue=metric_means.index,\n",
    "        legend=False,\n",
    "    )\n",
    "    plt.title(f\"{self.name}: RAGAS + Non-LLM Metrics (Mean)\", fontsize=16)\n",
    "    plt.ylabel(\"Mean Score\", fontsize=14)\n",
    "    plt.xlabel(\"Metrics\", fontsize=14)\n",
    "    plt.xticks(fontsize=12, rotation=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5da9e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import langdetect\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from langdetect import detect\n",
    "\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "import src.utilities as u\n",
    "import src.plots as p\n",
    "import src.constants as c\n",
    "c_stopwords = c.custom_stopwords\n",
    "\n",
    "from src.utilities import TextCleaner\n",
    "from src.credentials import AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import chromadb\n",
    "\n",
    "#!pip install langchain-community\n",
    "#!pip install openai\n",
    "#!pip install tiktoken\n",
    "\n",
    "df_train_1000 = pd.read_parquet('../data_mc1/data_processed/df_train_1000.parquet')\n",
    "df_train = pd.read_parquet('../data_mc1/data_processed/df_train_cleaned.parquet')\n",
    "df_eval = pd.read_parquet('../data_mc1/data_processed/cleantech_rag_evaluation_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caa4eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-3-large\", # gleich wie bei Embedden der Chunks\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    ")\n",
    "\n",
    "\n",
    "openai_llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4o-mini\",\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    temperature=0.0 \n",
    ")\n",
    "\n",
    "# Schritt 1: Bereite das Dataset vor\n",
    "df_ragas_input = df_test_embedd[[\n",
    "    \"question\", \n",
    "    \"relevant_text_llm\",    \n",
    "    \"answer_llm\", \n",
    "    \"answer\",\n",
    "    \"relevant_text\"\n",
    "]].copy()\n",
    "\n",
    "# Spalten umbenennen\n",
    "df_ragas_input = df_ragas_input.rename(columns={\n",
    "    \"relevant_text_llm\": \"retrieved_contexts\",\n",
    "    \"relevant_text\": \"reference\"\n",
    "})\n",
    "\n",
    "# Typen konvertieren\n",
    "for col in [\"question\", \"answer_llm\", \"answer\", \"reference\"]:\n",
    "    df_ragas_input[col] = df_ragas_input[col].astype(str)\n",
    "\n",
    "# Schritt 2: Hugging Face Dataset\n",
    "ragas_dataset = Dataset.from_pandas(df_ragas_input)\n",
    "\n",
    "# Schritt 3: Evaluation starten\n",
    "metrics = [faithfulness, answer_relevancy, context_precision]\n",
    "\n",
    "ragas_result = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=[faithfulness, answer_relevancy, context_precision],\n",
    "    embeddings=azure_embeddings,\n",
    "    llm=openai_llm\n",
    ")\n",
    "\n",
    "# Schritt 4: Ausgabe\n",
    "print(\"RAGAS Evaluation (mit GPT-4o-mini bewertet):\")\n",
    "print(ragas_result)\n",
    "\n",
    "# Schritt 5: Optional – Vergleich mit Goldstandard\n",
    "def simple_similarity(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "df_ragas_input[\"similarity_to_gold\"] = df_ragas_input.apply(\n",
    "    lambda row: simple_similarity(row[\"answer_llm\"], row[\"answer\"]), axis=1\n",
    ")\n",
    "\n",
    "mean_similarity = df_ragas_input[\"similarity_to_gold\"].mean()\n",
    "\n",
    "print(\"\\nVergleich answer_llm vs. Goldstandard:\")\n",
    "print(f\"Durchschnittliche Ähnlichkeit (answer_llm vs. answer): {mean_similarity:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
