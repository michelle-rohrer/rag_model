{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929acee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_retrieval_eval_chain(retriever, prompt_template, temperature, deployment_name):\n",
    "    \"\"\"\n",
    "    Baut eine modulare RetrievalQA-Chain für Evaluation.\n",
    "    \"\"\"\n",
    "    llm = AzureChatOpenAI(\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_version=\"2023-05-15\",\n",
    "        deployment_name=deployment_name,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    retrieval_chain = (\n",
    "        RunnableMap({\n",
    "            \"retrieved_documents\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "            \"question\": lambda x: x[\"question\"]\n",
    "        })\n",
    "        | RunnableMap({\n",
    "            \"retrieved_texts\": lambda x: [doc.page_content for doc in x[\"retrieved_documents\"]],\n",
    "            \"question\": lambda x: x[\"question\"]\n",
    "        })\n",
    "        | RunnableMap({\n",
    "            \"context\": lambda x: \"\\n\\n\".join(x[\"retrieved_texts\"]),\n",
    "            \"retrieved_texts\": lambda x: x[\"retrieved_texts\"],\n",
    "            \"question\": lambda x: x[\"question\"]\n",
    "        })\n",
    "        | RunnableMap({\n",
    "            \"llm_response\": lambda x: llm.invoke(prompt_template.format(context=x['context'], question=x['question'])),\n",
    "            \"retrieved_texts\": lambda x: x[\"retrieved_texts\"],\n",
    "            \"context\": lambda x: x[\"context\"],\n",
    "            \"question\": lambda x: x[\"question\"]\n",
    "        })\n",
    "    )\n",
    "    return retrieval_chain\n",
    "\n",
    "\n",
    "def run_batch_evaluation(df, retrieval_chain):\n",
    "    \"\"\"\n",
    "    Läuft alle Fragen im Batch (Eval_dataset) durch und sammelt die Ergebnisse.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        query = row[\"question\"]\n",
    "        relevant_text = row[\"relevant_text\"]\n",
    "        expected_answer = row[\"answer\"]\n",
    "\n",
    "        output = retrieval_chain.invoke({\"question\": query})\n",
    "\n",
    "        result = {\n",
    "            \"question\": query,\n",
    "            \"relevant_text\": relevant_text,\n",
    "            \"relevant_text_llm\": output[\"retrieved_texts\"],  # Liste der Chunks\n",
    "            \"retrieved_context\": output[\"context\"],          # Zusammengesetzter Kontext\n",
    "            \"answer\": expected_answer,\n",
    "            \"answer_llm\": output[\"llm_response\"]\n",
    "        }\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea2bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varianten an retriever\n",
    "retriever_similarity_k3 = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3} # top-k # Parameter zum anpassen\n",
    ")\n",
    "\n",
    "retriever_similarity_k5 = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5} # top-k # Parameter zum anpassen\n",
    ")\n",
    "\n",
    "retriever_mmr_k5_fetch20 = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",  # Maximal Marginal Relevance\n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 20}  # etwas mehr holen und diverse auswählen # Parameter zum anpassen\n",
    ")\n",
    "\n",
    "retriever_mmr_k7_fetch20 = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",  # Maximal Marginal Relevance\n",
    "    search_kwargs={\"k\": 7, \"fetch_k\": 20}  # etwas mehr holen und diverse auswählen # Parameter zum anpassen\n",
    ")\n",
    "\n",
    "# Varianten an Prompts\n",
    "prompt_simple = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based on the context provided.\n",
    "If you cannot find an answer, please state that you do not know.\n",
    "\n",
    "Kontext:\n",
    "{context}\n",
    "\n",
    "Frage:\n",
    "{question}\n",
    "\"\"\")\n",
    "\n",
    "prompt_chain_of_thought = ChatPromptTemplate.from_template(\"\"\"\n",
    "Given the context below, think carefully and step-by-step before answering the question.\n",
    "If the answer cannot be found explicitly, clearly state that.\n",
    "\n",
    "Kontext:\n",
    "{context}\n",
    "\n",
    "Frage:\n",
    "{question}\n",
    "\n",
    "Please explain your reasoning before giving a final answer.\n",
    "\"\"\")\n",
    "\n",
    "# Save der Varianten als Dictionary für leichteres speichern\n",
    "retrievers = {\n",
    "    \"similarity_k3\": retriever_similarity_k3,\n",
    "    \"similarity_k5\": retriever_similarity_k5,\n",
    "    \"mmr_k5\": retriever_mmr_k5_fetch20,\n",
    "    \"mmr_k7\": retriever_mmr_k7_fetch20\n",
    "}\n",
    "\n",
    "prompts = {\n",
    "    \"simple\": prompt_simple,\n",
    "    \"chain_of_thought\": prompt_chain_of_thought\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a8602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "temperature=0.5 # Parameter zum anpassen \n",
    "\n",
    "for retriever_name, retriever in retrievers.items():\n",
    "    for prompt_name, prompt in prompts.items():\n",
    "        \n",
    "        print(f\"Starte Kombination: {retriever_name} + {prompt_name}\")\n",
    "        \n",
    "        # Chain bauen\n",
    "        retrieval_eval_chain = build_retrieval_eval_chain(\n",
    "            retriever=retriever,\n",
    "            prompt_template=prompt,\n",
    "            temperature=temperature,   \n",
    "            deployment_name=\"gpt-4o\" # zur eval ev. mini version verwenden\n",
    "        )\n",
    "\n",
    "        # Evaluation durchführen\n",
    "        df_result = run_batch_evaluation(df_eval, retrieval_eval_chain)\n",
    "        \n",
    "        # Sicherstellen, dass answer_llm als reiner Text gespeichert wird\n",
    "        df_result[\"answer_llm\"] = df_result[\"answer_llm\"].apply(lambda x: x.content if hasattr(x, \"content\") else x)\n",
    "\n",
    "        # Ergebnis speichern\n",
    "        temperature_str = str(temperature).replace(\".\", \"\")\n",
    "        file_name = f\"../data_mc1/data_eval/df_results_{retriever_name}_{prompt_name}_temp{temperature_str}.parquet\"\n",
    "        df_result.to_parquet(file_name, index=False)\n",
    "        print(f\"Gespeichert unter: {file_name}\")\n",
    "        \n",
    "        results.append(df_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778efd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_print = pd.read_parquet(\"../data_mc1/data_eval/df_results_similarity_simple.parquet\")\n",
    "df_test_print.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
